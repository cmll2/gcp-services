# Dataproc

**Keywords**: `Hadoop`, `Spark`, `Big Data`, `Cluster`, `Managed`, `ETL`, `Data Lake`, `GCP`, `Serverless`, `Analytics`

---

## üß† Description G√©n√©rale

**Dataproc** est un service manag√© de Google Cloud pour le d√©ploiement, la gestion et le scaling de clusters Apache Hadoop, Spark, Hive et d‚Äôautres outils Big Data. Il permet d‚Äôex√©cuter des traitements de donn√©es massifs de fa√ßon flexible et √©conomique.

Caract√©ristiques principales :
- Cr√©ation et suppression rapide de clusters √† la demande.
- Facturation √† la seconde, scaling automatique.
- Int√©gration native avec Cloud Storage, BigQuery, Cloud Logging, etc.
- Support des jobs Spark, Hadoop, Hive, Presto, PySpark, etc.
- Gestion simplifi√©e des d√©pendances et des configurations.

---

## üß∞ Composants Principaux

### 1. **Clusters**
- Groupes de VM configur√©es pour ex√©cuter des workloads Big Data.
- Personnalisation des ressources (CPU, RAM, disques, nombre de n≈ìuds).

### 2. **Jobs**
- Ex√©cution de traitements Spark, Hadoop, Hive, PySpark, etc.
- Soumission via CLI, API, ou interface web.

### 3. **Autoscaling**
- Ajustement automatique du nombre de n≈ìuds selon la charge.

### 4. **Int√©gration GCP**
- Connexion directe √† Cloud Storage, BigQuery, Cloud Logging, Cloud Monitoring.

### 5. **Initialisation & Scripts**
- Scripts d‚Äôinitialisation pour installer des d√©pendances ou configurer le cluster.

---

## üßë‚Äçüíº Cas d‚ÄôUsage

| Cas d‚Äôusage                         | Description |
|------------------------------------|-------------|
| ETL Big Data                        | Traitement et transformation de gros volumes de donn√©es |
| Machine Learning distribu√©          | Entra√Ænement de mod√®les ML sur Spark |
| Data Lake                           | Pr√©paration et analyse de donn√©es non structur√©es |
| Migration Hadoop                    | D√©placement de workloads Hadoop on-premise vers le cloud |
| Analyse ad hoc                      | Ex√©cution ponctuelle de jobs analytiques |

---

## üßë‚Äçüî¨ Exemple d‚ÄôArchitecture : Dataproc sur GCP

1. **Stockage** : Donn√©es sources sur [Cloud Storage](../Storage/storage.md).
2. **Cluster Dataproc** : Cr√©ation √† la demande pour ex√©cuter les jobs.
3. **Traitement** : Jobs Spark/Hadoop/PySpark soumis au cluster.
4. **Sortie** : R√©sultats stock√©s sur Cloud Storage ou [BigQuery](../BigQuery/bigquery.md).
5. **Monitoring** : Supervision via [Cloud Monitoring](../CloudMonitoring/cloudmonitoring.md) et [Cloud Logging](../CloudLogging/cloudlogging.md).

---

# üöÄ Commandes / Code utiles

### Exemple : Cr√©er un cluster Dataproc

```bash
gcloud dataproc clusters create my-cluster \
  --region=europe-west1 \
  --zone=europe-west1-b \
  --single-node \
  --image-version=2.0-debian10
```

### Exemple : Soumettre un job PySpark

```bash
gcloud dataproc jobs submit pyspark gs://your-bucket/scripts/my_job.py \
  --cluster=my-cluster \
  --region=europe-west1
```

## ‚úÖ Bonnes Pratiques

- Cr√©er et supprimer les clusters √† la demande pour optimiser les co√ªts.
- Utiliser Cloud Storage comme data lake pour s√©parer stockage et calcul.
- Automatiser les jobs avec des workflows ou des scripts d‚Äôinitialisation.
- Superviser les clusters et jobs avec Cloud Monitoring et Logging.
- S√©curiser l‚Äôacc√®s aux donn√©es et clusters avec IAM et VPC.
