# Explainable AI

**Keywords**: `Explainability`, `Interpretability`, `Machine Learning`, `SHAP`, `Integrated Gradients`, `Model Transparency`, `GCP`, `Responsible AI`

---

## üß† Description G√©n√©rale

**Explainable AI** est un ensemble d'outils et de techniques propos√©s par Google Cloud pour rendre les mod√®les de Machine Learning plus transparents et interpr√©tables. Il aide les utilisateurs √† comprendre comment et pourquoi un mod√®le prend des d√©cisions, ce qui est essentiel pour des domaines sensibles comme la finance, la sant√© ou les assurances.

Caract√©ristiques principales :
- Explications locales et globales des pr√©dictions.
- Supporte les mod√®les AutoML et les mod√®les personnalis√©s.
- M√©thodes d'explicabilit√© comme **SHAP**, **Integrated Gradients**, et **XRAI**.
- Int√©gration avec Vertex AI pour superviser les mod√®les en production.

---

## üß∞ Composants Principaux

### 1. **Explications Locales**
- Fournissent des informations sur les caract√©ristiques qui influencent une pr√©diction sp√©cifique.
- Exemple : Pourquoi un pr√™t a √©t√© approuv√© ou refus√©.

### 2. **Explications Globales**
- Fournissent une vue d'ensemble des caract√©ristiques les plus importantes pour le mod√®le.
- Exemple : Identifier les variables les plus influentes dans un mod√®le de pr√©diction de churn.

### 3. **M√©thodes d'Explicabilit√©**
- **SHAP (SHapley Additive exPlanations)** : Attribution de l'importance des caract√©ristiques en utilisant la th√©orie des jeux.
- **Integrated Gradients** : Analyse des gradients pour mesurer l'impact des caract√©ristiques.
- **XRAI (eXplainable Region-based AI)** : Explicabilit√© bas√©e sur les r√©gions pour les mod√®les de vision.

### 4. **Int√©gration avec Vertex AI**
- G√©n√©ration d'explications pour les mod√®les d√©ploy√©s sur Vertex AI.
- Monitoring des d√©rives des caract√©ristiques pour d√©tecter les biais en production.

---

## üßë‚Äçüíº Cas d‚ÄôUsage

| Cas d‚Äôusage                         | Description |
|------------------------------------|-------------|
| Finance                             | Comprendre pourquoi un pr√™t ou une assurance a √©t√© approuv√©/refus√© |
| Sant√©                               | Identifier les facteurs influen√ßant un diagnostic ou une pr√©diction m√©dicale |
| Marketing                           | Analyser les variables influen√ßant la pr√©diction de churn ou de conversion |
| D√©tection de fraude                 | Comprendre les d√©cisions des mod√®les de d√©tection d'anomalies |
| Vision par ordinateur               | Identifier les r√©gions d'une image qui influencent une pr√©diction |

---

## üßë‚Äçüî¨ Exemple d‚ÄôArchitecture : Explainable AI avec Vertex AI

1. **Source de donn√©es** : Donn√©es tabulaires, images ou texte.
2. **Mod√®le** : Entra√Ænement d'un mod√®le avec AutoML ou un mod√®le personnalis√©.
3. **Explicabilit√©** : G√©n√©ration d'explications locales et globales avec Explainable AI.
4. **D√©ploiement** : D√©ploiement du mod√®le sur Vertex AI.
5. **Monitoring** : Supervision des d√©rives des caract√©ristiques et des performances du mod√®le.

---

# üöÄ Commandes / Code utiles

### Exemple : G√©n√©rer des explications pour un mod√®le d√©ploy√©

```python
from google.cloud import aiplatform

aiplatform.init(project="your-project-id", location="us-central1")

endpoint = aiplatform.Endpoint(endpoint_name="projects/your-project-id/locations/us-central1/endpoints/your-endpoint-id")

instances = [
    {"feature1": 5.1, "feature2": 3.5, "feature3": 1.4, "feature4": 0.2}
]

response = endpoint.explain(instances=instances)
for explanation in response.explanations:
    print("Explications locales :")
    for attribution in explanation.attributions:
        print(f"Caract√©ristique : {attribution.feature_name}, Importance : {attribution.attribution_score}")
```

### Exemple : Activer Explainable AI lors du d√©ploiement d'un mod√®le

```python
from google.cloud import aiplatform

aiplatform.init(project="your-project-id", location="us-central1")

model = aiplatform.Model(model_name="projects/your-project-id/locations/us-central1/models/your-model-id")

endpoint = model.deploy(
    deployed_model_display_name="explainable-model",
    machine_type="n1-standard-4",
    explanation_metadata={
        "inputs": {
            "feature1": {"input_tensor_name": "feature1", "encoding": "BAG_OF_FEATURES"},
            "feature2": {"input_tensor_name": "feature2", "encoding": "IDENTITY"},
        },
        "outputs": {"output_tensor_name": "predicted_value"},
    },
    explanation_parameters={
        "sampled_shapley_attribution": {"path_count": 10}
    },
)
print(f"Mod√®le d√©ploy√© avec explicabilit√© activ√©e : {endpoint.resource_name}")
```

## ‚úÖ Bonnes Pratiques

- Utiliser des explications locales pour analyser des pr√©dictions sp√©cifiques.
- Superviser les d√©rives des caract√©ristiques pour d√©tecter les biais en production.
- Activer Explainable AI d√®s le d√©ploiement pour les cas sensibles (finance, sant√©).
- Tester diff√©rentes m√©thodes d'explicabilit√© (SHAP, Integrated Gradients) pour - choisir la plus adapt√©e.
- Documenter les r√©sultats des explications pour am√©liorer la transparence aupr√®s des parties prenantes.