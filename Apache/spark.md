# Apache Spark

**Keywords**: `Big Data`, `Spark`, `Distributed Computing`, `Batch`, `Streaming`, `ETL`, `Machine Learning`, `Analytics`, `Open Source`, `Cluster`

---

## üß† Description G√©n√©rale

**Apache Spark** est un moteur open source de traitement distribu√© pour le Big Data. Il permet d‚Äôex√©cuter des traitements batch, du streaming, du machine learning et de l‚Äôanalytique avanc√©e sur de grands volumes de donn√©es, en m√©moire ou sur disque.

Caract√©ristiques principales :
- Traitement distribu√© en m√©moire pour des performances √©lev√©es.
- Support du batch, du streaming, du ML (MLlib) et du SQL (Spark SQL).
- API unifi√©es en Python (PySpark), Scala, Java, R.
- Int√©gration avec HDFS, S3, Hive, Cassandra, HBase, etc.
- D√©ploiement sur clusters on-premise, cloud, Kubernetes, YARN, Mesos.

---

## üß∞ Composants Principaux

### 1. **Spark Core**
- Moteur d‚Äôex√©cution distribu√©, gestion des t√¢ches et de la m√©moire.

### 2. **Spark SQL**
- Requ√™tes SQL, DataFrames, int√©gration avec Hive Metastore.

### 3. **Spark Streaming**
- Traitement de flux de donn√©es en temps r√©el (micro-batching).

### 4. **MLlib**
- Librairie de machine learning distribu√©e (classification, clustering, r√©gression‚Ä¶).

### 5. **GraphX**
- Traitement de graphes distribu√©s.

---

## üßë‚Äçüíº Cas d‚ÄôUsage

| Cas d‚Äôusage                         | Description |
|------------------------------------|-------------|
| ETL Big Data                        | Pr√©paration et transformation de donn√©es massives |
| Streaming temps r√©el                | Analyse de flux (logs, IoT, clickstream‚Ä¶) |
| Machine Learning distribu√©          | Entra√Ænement de mod√®les ML sur de gros volumes |
| SQL analytique                      | Requ√™tes SQL sur des data lakes |
| Traitement de graphes               | Analyse de r√©seaux sociaux, recommandations |

---

## üßë‚Äçüî¨ Exemple d‚ÄôArchitecture : Spark sur cluster

1. **Stockage** : Donn√©es sur HDFS, S3, Cloud Storage, etc.
2. **Cluster Spark** : D√©ploiement sur YARN, Kubernetes, Dataproc, EMR, etc.
3. **Traitement** : Jobs Spark batch ou streaming soumis au cluster.
4. **Sortie** : R√©sultats sur HDFS, S3, BigQuery, bases SQL, etc.
5. **Monitoring** : Suivi via Spark UI, Cloud Monitoring, Prometheus, Grafana.

---

# üöÄ Commandes / Code utiles

### Exemple : Lancer un job PySpark

```bash
spark-submit --master yarn --deploy-mode cluster my_job.py
```

### Exemple : Traitement de donn√©es avec PySpark

```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("MyApp").getOrCreate()
df = spark.read.csv("s3://my-bucket/data.csv", header=True, inferSchema=True)
df.groupBy("category").count().show()
```

### Exemple : Streaming avec PySpark

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("streaming").getOrCreate()
df = spark.readStream.format("socket").option("host", "localhost").option("port", 9999).load()
df.writeStream.format("console").start().awaitTermination()
```

## ‚úÖ Bonnes Pratiques

- Utiliser le partitionnement et le cache pour optimiser les performances.
- Superviser les jobs avec Spark UI et des outils externes.
- G√©rer les d√©pendances avec des fichiers requirements ou jars.
- Adapter le nombre de partitions √† la taille des donn√©es.
- Nettoyer les ressources (jobs, clusters) apr√®s usage pour ma√Ætriser les co√ªts.
- Privil√©gier DataFrames/Datasets pour les traitements complexes.

