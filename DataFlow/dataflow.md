# Dataflow

**Keywords**: `ETL`, `Streaming`, `Batch`, `Apache Beam`, `Data Pipelines`, `Serverless`, `GCP`, `Managed Services`

---

## üß† Description G√©n√©rale

**Dataflow** est un service enti√®rement g√©r√© de Google Cloud pour le traitement de donn√©es en mode batch et streaming. Bas√© sur le framework **Apache Beam**, il permet de cr√©er des pipelines de donn√©es scalables et performants.

Caract√©ristiques principales :
- Traitement unifi√© des donn√©es en **batch** et en **streaming**.
- √âvolutivit√© automatique pour g√©rer des volumes de donn√©es variables.
- Int√©gration avec d'autres services GCP comme Pub/Sub, BigQuery, Cloud Storage.
- Mod√®le de programmation bas√© sur Apache Beam.

---

## üß∞ Composants Principaux

### 1. **Pipelines**
- D√©finissent le flux de donn√©es √† travers des √©tapes de transformation.
- √âcrits en Python, Java ou SQL via Apache Beam.

### 2. **Sources de donn√©es**
- Supporte des sources comme Pub/Sub, Cloud Storage, BigQuery, Kafka, bases de donn√©es.

### 3. **Transformations**
- **ParDo** : Transformation √©l√©ment par √©l√©ment.
- **GroupByKey** : Regroupement des donn√©es par cl√©.
- **Windowing** : Regroupement des donn√©es en fen√™tres temporelles.

### 4. **Sinks (Cibles)**
- Export des donn√©es transform√©es vers des destinations comme BigQuery, Cloud Storage, Pub/Sub.

### 5. **Templates**
- Pipelines pr√©configur√©s pour des cas d'usage courants (ETL, migration de donn√©es).

---

## üßë‚Äçüíº Cas d‚ÄôUsage

| Cas d‚Äôusage                         | Description |
|------------------------------------|-------------|
| ETL (Extract, Transform, Load)     | Extraction de donn√©es depuis des sources multiples, transformation et chargement dans BigQuery |
| Traitement de logs en temps r√©el    | Ingestion de logs via Pub/Sub et analyse en streaming |
| Migration de donn√©es                | D√©placement de donn√©es entre syst√®mes (Cloud Storage vers BigQuery) |
| Analyse IoT                         | Traitement en temps r√©el des donn√©es IoT via Pub/Sub |
| D√©tection de fraudes                | Analyse en streaming pour identifier des anomalies dans les transactions |

---

## üßë‚Äçüî¨ Exemple d‚ÄôArchitecture : Pipeline Dataflow

1. **Source de donn√©es** : [Pub/Sub](../PubSub/pubsub.md) ou [Cloud Storage](../Storage/storage.md).
2. **Pipeline Dataflow** : Transformations des donn√©es avec Apache Beam.
3. **Cible** : [BigQuery](../BigQuery/bigquery.md) ou [Cloud Storage](../Storage/storage.md).
4. **Monitoring** : Supervision des m√©triques via [Cloud Monitoring](../CloudMonitoring/cloudmonitoring.md).

---

# üöÄ Commandes / Code utiles

### Exemple : Pipeline Dataflow en Python

```python
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions

pipeline_options = PipelineOptions(
    project='your-project-id',
    region='us-central1',
    runner='DataflowRunner',
    temp_location='gs://your-bucket/temp',
    staging_location='gs://your-bucket/staging'
)

def transform_data(element):
    # Exemple de transformation
    return element.upper()

with beam.Pipeline(options=pipeline_options) as p:
    (p
     | 'Lire depuis Pub/Sub' >> beam.io.ReadFromPubSub(topic='projects/your-project-id/topics/your-topic')
     | 'Transformer les donn√©es' >> beam.Map(transform_data)
     | '√âcrire dans BigQuery' >> beam.io.WriteToBigQuery(
         table='your-project-id:your_dataset.your_table',
         schema='field1:STRING, field2:INTEGER',
         write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND
     ))
```

## ‚úÖ Bonnes Pratiques

- Utiliser des fen√™tres temporelles pour regrouper les donn√©es en streaming.
- Configurer des pipelines templates pour r√©utiliser des workflows courants.
- Superviser les pipelines avec Cloud Monitoring pour d√©tecter les erreurs.
- Optimiser les co√ªts en ajustant les param√®tres de parall√©lisme.
- Tester localement avec le DirectRunner avant de d√©ployer sur Dataflow.