# BigQuery

**Keywords**: `Data Warehouse`, `Analytics`, `SQL`, `ETL`, `Serverless`, `Managed Services`, `BI`

---

## üß† Description G√©n√©rale

**BigQuery** est un entrep√¥t de donn√©es enti√®rement g√©r√© et serverless propos√© par Google Cloud. Il permet d'ex√©cuter des requ√™tes SQL analytiques sur des volumes massifs de donn√©es en quelques secondes.

Caract√©ristiques principales :
- Analyse rapide et √©volutive.
- Mod√®le de tarification bas√© sur l'utilisation (paiement √† la requ√™te ou forfait).
- Int√©gration avec des outils BI comme Looker, Tableau, Data Studio.
- Support des donn√©es structur√©es et semi-structur√©es (JSON, Avro, Parquet).

---

## üß∞ Composants Principaux

### 1. **Tables et Sch√©mas**
- Stockage des donn√©es dans des tables partitionn√©es ou non.
- Supporte les formats CSV, JSON, Avro, Parquet, ORC.

### 2. **SQL Standard**
- Langage SQL standardis√© pour les requ√™tes.
- Supporte les fonctions analytiques, les jointures complexes, les sous-requ√™tes.

### 3. **BigQuery ML**
- Entra√Ænement de mod√®les ML directement avec SQL.
- Mod√®les support√©s : r√©gression, classification, clustering, s√©ries temporelles.

### 4. **BigQuery BI Engine**
- Moteur en m√©moire pour des tableaux de bord interactifs.
- Optimis√© pour les outils BI.

### 5. **BigQuery Omni**
- Analyse des donn√©es stock√©es sur AWS ou Azure sans les d√©placer.

### 6. **Streaming**
- Ingestion en temps r√©el des donn√©es via l'API de streaming.

### 7. **BigQuery Data Transfer Service**
- Automatisation des imports de donn√©es depuis des sources comme Google Ads, YouTube, etc.

---

## üßë‚Äçüíº Cas d‚ÄôUsage

| Cas d‚Äôusage                         | Description |
|------------------------------------|-------------|
| Analyse des ventes                 | Requ√™tes SQL sur des donn√©es transactionnelles |
| Reporting BI                       | Int√©gration avec Looker ou Data Studio |
| Analyse de logs                    | Ingestion de logs en temps r√©el via Cloud Logging |
| Pr√©diction de tendances            | BigQuery ML pour s√©ries temporelles |
| Analyse multi-cloud                | BigQuery Omni pour AWS/Azure |
| D√©tection d'anomalies              | Clustering avec BigQuery ML |

---

## üßë‚Äçüî¨ Exemple d‚ÄôArchitecture : Analyse avec BigQuery

1. **Source de donn√©es** : [Cloud Storage](../CloudStorage/cloudstorage.md) ou [Pub/Sub](../PubSub/pubsub.md)
2. **Ingestion** : [Dataflow](../Dataflow/dataflow.md) ou API de streaming
3. **Stockage** : Tables BigQuery partitionn√©es
4. **Analyse** : SQL ou BigQuery ML
5. **Visualisation** : [Looker](../Looker/looker.md) ou [Data Studio](../DataStudio/datastudio.md)

---

# üöÄ Commandes / Code utiles

### Exemple : Requ√™te SQL simple

```sql
SELECT
  customer_id,
  SUM(total_amount) AS total_spent
FROM
  `project_id.dataset_id.sales`
WHERE
  transaction_date BETWEEN '2023-01-01' AND '2023-12-31'
GROUP BY
  customer_id
ORDER BY
  total_spent DESC
LIMIT 10;
```

### Exemple : Chargement de donn√©es depuis Cloud Storage

```python
from google.cloud import bigquery

client = bigquery.Client()

table_id = "your-project.your_dataset.your_table"

job_config = bigquery.LoadJobConfig(
    source_format=bigquery.SourceFormat.CSV,
    skip_leading_rows=1,
    autodetect=True,
)

uri = "gs://your-bucket-name/your-file.csv"

load_job = client.load_table_from_uri(
    uri, table_id, job_config=job_config
)

load_job.result()

print(f"Table {table_id} charg√©e avec succ√®s.")
```

## ‚úÖ Bonnes Pratiques

- Partitionner les tables pour optimiser les performances.
- Utiliser des clusters pour acc√©l√©rer les requ√™tes sur des colonnes sp√©cifiques.
- Activer les quotas et les budgets pour √©viter les d√©passements de co√ªts.
- Automatiser les imports avec BigQuery Data Transfer Service.
- Utiliser BigQuery ML pour des analyses pr√©dictives rapides.